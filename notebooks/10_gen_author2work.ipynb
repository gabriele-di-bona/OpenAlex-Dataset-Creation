{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dfd5bfc-2e7e-4d1a-bc7b-66cc0f4cacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\") # resets notebook directory to repository root folder (DO ONLY ONCE!)\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pypq\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c44ecd-4340-488c-9e56-858be877adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyArrow strings!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Add utils directory in the list of directories to look for packages to import\n",
    "sys.path.insert(0, os.path.join(os.getcwd(),'utils'))\n",
    "from read_parquet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c0d3d8-6e2b-4fdf-a1eb-d6c651ba39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "works_by_topic_parquet_folder = \"data/works_by_topic_parquet/\"\n",
    "author2work_by_topic_csv_folder = \"data/author2work_by_topic_csv/\"\n",
    "os.makedirs(author2work_by_topic_csv_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a537cd-98ae-4532-899d-c215eb46faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [topic[:-8] for topic in os.listdir(works_by_topic_parquet_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec3ca42-301a-43a3-b6fc-a32751f15278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4516"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25ca93b-1814-427a-ae4a-83d88c41de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_author2work_df(works_path, out_path, batch_size=1000, do_print=True):\n",
    "    # Build author2work by processing works in batches to limit memory.\n",
    "    # Append to CSV (header only on first write).\n",
    "    df = read_parquet(works_path, columns=['id', 'date', 'authors'], quiet=True)\n",
    "    total = len(df)\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    first_write = True\n",
    "\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = min(start + batch_size, total)\n",
    "        batch = df.iloc[start:end].copy()\n",
    "\n",
    "        batch = batch[batch['authors'].apply(lambda x: len(x) > 0)]\n",
    "        batch['authors'] = batch['authors'].apply(lambda x: x.split(';'))\n",
    "        batch = batch.explode('authors')\n",
    "    \n",
    "        def parse_author(x):\n",
    "            parts = x.split('_')\n",
    "            # primary_affiliation = ''\n",
    "            affs = ''\n",
    "            author_id = ''\n",
    "            is_corr = 'F'\n",
    "            if len(parts) > 2:\n",
    "                author_id = \"_\".join(parts[:-2]) if len(parts) > 2 else ''\n",
    "                affs = parts[-2]#.split('|') if parts[1] else []\n",
    "                # if len(affs) > 0:\n",
    "                #     primary_affiliation = affs[0]\n",
    "                is_corr = parts[-1]\n",
    "            return author_id, affs, is_corr\n",
    "\n",
    "        parsed = batch['authors'].apply(parse_author)\n",
    "        batch['author_id'] = parsed.apply(lambda x: x[0])\n",
    "        # batch['primary_affiliation'] = parsed.apply(lambda x: x[1])\n",
    "        batch['affiliations'] = parsed.apply(lambda x: x[1])\n",
    "        batch['is_corresponding'] = parsed.apply(lambda x: x[2])\n",
    "\n",
    "        batch = batch.rename(columns={'id': 'work_id'})\n",
    "        batch = batch[['author_id', 'work_id', 'date', 'affiliations', 'is_corresponding']]\n",
    "\n",
    "        batch.to_csv(out_path, mode='a', header=first_write, index=False)\n",
    "        first_write = False\n",
    "\n",
    "        if do_print:\n",
    "            print(f'Processed rows {start}-{end} / {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26644e50-c136-44c7-81b7-28976d7b7c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3649d17f29ee4a468b66c98d6db9a7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.listdir(author2work_by_topic_csv_folder):  # listdir returns [] if empty\n",
    "    existing = os.listdir(author2work_by_topic_csv_folder)\n",
    "    raise RuntimeError(\n",
    "        f\"Destination folder '{author2work_by_topic_csv_folder}' is not empty! \"\n",
    "        f\"Found {len(existing)} file(s), e.g., {existing[:3]}. \"\n",
    "        f\"Please clean it before running this script to avoid appending duplicates.\"\n",
    "    )\n",
    "\n",
    "for topic in tqdm(topics):\n",
    "    origin_works_parquet_file_path = works_by_topic_parquet_folder+topic+\".parquet\"\n",
    "    destination_topic_csv_file_path = author2work_by_topic_csv_folder+topic+\".csv\"\n",
    "    generate_author2work_df(origin_works_parquet_file_path, destination_topic_csv_file_path, batch_size = 10000, do_print = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef69cbba-d0d3-4632-bc80-850a375ca5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv2parquet(csv_file_path, parquet_file_path, sort_by = None, separator=',', compression='brotli', do_peek = True, do_print = True):\n",
    "    os.makedirs(os.path.dirname(parquet_file_path), exist_ok=True)\n",
    "    df = pl.read_csv(csv_file_path, separator=separator, infer_schema_length=1000)\n",
    "    if sort_by is not None:\n",
    "        # sort first by first column name in title, than second if equal, etc.\n",
    "        df = df.sort(sort_by, descending = True)\n",
    "    df.write_parquet(parquet_file_path, compression=compression)\n",
    "    if do_print:\n",
    "        print(f\"Successfully converted {csv_file_path} to {parquet_file_path}.\")\n",
    "    if do_peek:\n",
    "        if do_print:\n",
    "            print(\"Here's a peek.\")\n",
    "        peek_parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f84b778-66e2-4d58-9d41-74dca20de23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "author2work_by_topic_parquet_folder = \"data/author2work_by_topic_parquet/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdcfb77a-a798-48fb-9714-3129796db4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344c2644612a49fda99fa471df1e4149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in tqdm(topics):\n",
    "    csv_file_path = author2work_by_topic_csv_folder+topic+\".csv\"\n",
    "    parquet_file_path = author2work_by_topic_parquet_folder+topic+\".parquet\"\n",
    "    convert_csv2parquet(csv_file_path, parquet_file_path, sort_by = None, separator = ',', do_peek = False, do_print = False)\n",
    "peek_parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32a17000-8564-4053-8b56-a9d842fa2e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 'T10091'\n",
      "Path: 'data/author2work_by_topic_parquet/T10091.parquet'\n",
      "Files: 1\n",
      "Rows: 899,151\n",
      "Schema:\n",
      "    author_id: large_string\n",
      "    work_id: large_string\n",
      "    date: large_string\n",
      "    affiliations: large_string\n",
      "    is_corresponding: large_string\n",
      "5 random rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>work_id</th>\n",
       "      <th>date</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>is_corresponding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shi, Lining</td>\n",
       "      <td>W7083686573</td>\n",
       "      <td>2030-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pinthong, Nattapon</td>\n",
       "      <td>W6963238420</td>\n",
       "      <td>2028-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Klaus Reuter</td>\n",
       "      <td>W6929729901</td>\n",
       "      <td>2028-01-01</td>\n",
       "      <td>I4210132734</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cyril Hanus</td>\n",
       "      <td>W6929729901</td>\n",
       "      <td>2028-01-01</td>\n",
       "      <td>I154526488|I4210130152</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mateusz Sikora</td>\n",
       "      <td>W6929729901</td>\n",
       "      <td>2028-01-01</td>\n",
       "      <td>I126596746</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author_id      work_id        date            affiliations  \\\n",
       "0         Shi, Lining  W7083686573  2030-01-01                    None   \n",
       "1  Pinthong, Nattapon  W6963238420  2028-01-01                    None   \n",
       "2        Klaus Reuter  W6929729901  2028-01-01             I4210132734   \n",
       "3         Cyril Hanus  W6929729901  2028-01-01  I154526488|I4210130152   \n",
       "4      Mateusz Sikora  W6929729901  2028-01-01              I126596746   \n",
       "\n",
       "  is_corresponding  \n",
       "0                T  \n",
       "1                T  \n",
       "2                T  \n",
       "3                F  \n",
       "4                F  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peek_parquet(parquet_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_enrico-gt_rocky] *",
   "language": "python",
   "name": "conda-env-env_enrico-gt_rocky-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

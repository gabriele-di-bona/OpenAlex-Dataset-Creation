{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ece3e34-2364-4a4c-b79a-39c4f595106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.chdir(\"../\") # resets notebook directory to repository root folder (DO ONLY ONCE!)\n",
    "import gzip\n",
    "import tqdm\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "# import pyarrow as pa\n",
    "\n",
    "def flatten(matrix):\n",
    "    return [item for row in matrix for item in row]\n",
    "\n",
    "def simpleId(text):\n",
    "    try:\n",
    "        y=text.split('/')[-1]\n",
    "    except:\n",
    "        y='NONE'\n",
    "    return y\n",
    "\n",
    "def flush_buffer(lines, output_dir):\n",
    "    df = pl.DataFrame(lines, orient = \"row\", schema = columns)\n",
    "\n",
    "    out_path = os.path.join(output_dir, f\"topics.csv\")\n",
    "    file_exists = os.path.isfile(out_path)\n",
    "    # # USING pandas\n",
    "    # df.write_csv(\n",
    "    #     out_path,\n",
    "    #     has_header=not file_exists,\n",
    "    #     separator=',',\n",
    "    #     append=file_exists\n",
    "    # )\n",
    "    # USING polars\n",
    "    csv_str = df.write_csv(separator=';', include_header=not file_exists)\n",
    "    with open(out_path, 'a', encoding='utf-8') as f:\n",
    "        f.write(csv_str)\n",
    "    lines = []\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3119ba4-9fe5-4417-a84a-93881262fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_subfolder = \"data/openalex-snapshot/data/topics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11be39b-6a30-4091-9996-b23230f08b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58 subfolders\n"
     ]
    }
   ],
   "source": [
    "listdir=[subfolder for subfolder in sorted(os.listdir(snapshot_subfolder)) if 'updated' in subfolder]\n",
    "print(f\"Found {len(listdir)} subfolders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bb17e5-0552-4e79-9cc8-73bae3220744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part_000.gz']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(snapshot_subfolder+listdir[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2441e2-c787-4356-bbff-f5b63127d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58 files\n"
     ]
    }
   ],
   "source": [
    "files=flatten([[snapshot_subfolder+listdir[a]+'/'+i for i in os.listdir(snapshot_subfolder+listdir[a]) if 'part' in i] for a in range(len(listdir))])\n",
    "print(f\"Found {len(files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c3e228-d1a9-4db0-94c9-028ab96d89f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example file: data/openalex-snapshot/data/topics/updated_date=2024-03-04/part_000.gz\n"
     ]
    }
   ],
   "source": [
    "print(\"Example file:\", files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef891bd5-c9a5-4ac8-beed-b79517489ec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:00<00:00, 124.55it/s]\n"
     ]
    }
   ],
   "source": [
    "destination_csv_folder = \"data/topics/\"\n",
    "os.makedirs(destination_csv_folder, exist_ok=True)\n",
    "\n",
    "# before doing any damage, check if there are files in the destination folder and stop if there are, \n",
    "# because if there are already csv, it will append and might cause many duplicates\n",
    "if os.listdir(destination_csv_folder):  # listdir returns [] if empty\n",
    "    existing = os.listdir(destination_csv_folder)\n",
    "    raise RuntimeError(\n",
    "        f\"Destination folder '{destination_csv_folder}' is not empty! \"\n",
    "        f\"Found {len(existing)} file(s), e.g., {existing[:3]}. \"\n",
    "        f\"Please clean it before running this script to avoid appending duplicates.\"\n",
    "    )\n",
    "\n",
    "columns=['topic_id','display_name','description','keywords','wikipedia',\n",
    "                 'subfield_id','subfield_name','field_id','field_name',\n",
    "                 'domain_id','domain_name','sibling_ids', 'works_count', 'cited_by_count']\n",
    "\n",
    "lines=[]\n",
    "for gzfile in tqdm(files):\n",
    "    with gzip.open(gzfile, 'rt') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                topic_id = data[\"id\"].split(\"/\")[-1]  # Extract topic ID\n",
    "                display_name = data.get(\"display_name\", \"\")\n",
    "                description = data.get(\"description\", \"\")\n",
    "                keywords = \"_\".join(data.get(\"keywords\", []))  # Keep it as a list of separate strings\n",
    "                \n",
    "                # Extract subfield, field, and domain\n",
    "                subfield = data.get(\"subfield\", {})\n",
    "                subfield_id = subfield.get(\"id\", \"\").split(\"/\")[-1] if subfield else \"\"\n",
    "                subfield_name = subfield.get(\"display_name\", \"\")\n",
    "                \n",
    "                field = data.get(\"field\", {})\n",
    "                field_id = field.get(\"id\", \"\").split(\"/\")[-1] if field else \"\"\n",
    "                field_name = field.get(\"display_name\", \"\")\n",
    "                \n",
    "                domain = data.get(\"domain\", {})\n",
    "                domain_id = domain.get(\"id\", \"\").split(\"/\")[-1] if domain else \"\"\n",
    "                domain_name = domain.get(\"display_name\", \"\")\n",
    "                \n",
    "                # Extract Wikipedia link if present\n",
    "                wikipedia = data.get(\"ids\", \"\").get(\"wikipedia\", \"\")\n",
    "                \n",
    "                # Extract sibling IDs\n",
    "                siblings = data.get(\"siblings\", [])\n",
    "                sibling_ids = \"_\".join([sibling[\"id\"].split(\"/\")[-1] for sibling in siblings])\n",
    "                \n",
    "                works_count = data.get(\"works_count\", -1)\n",
    "                cited_by_count = data.get(\"cited_by_count\", -1)\n",
    "                # Append extracted data to the list\n",
    "                lines.append([\n",
    "                    topic_id,\n",
    "                    display_name,\n",
    "                    description,\n",
    "                    keywords,\n",
    "                    wikipedia,\n",
    "                    subfield_id,\n",
    "                    subfield_name,\n",
    "                    field_id,\n",
    "                    field_name,\n",
    "                    domain_id,\n",
    "                    domain_name,\n",
    "                    sibling_ids,\n",
    "                    works_count,\n",
    "                    cited_by_count\n",
    "                ])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Errore nel parsing della riga: {e}\")\n",
    "            if len(lines) > 1000:\n",
    "                lines = flush_buffer(lines, destination_csv_folder)\n",
    "lines = flush_buffer(lines, destination_csv_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_enrico-gt_rocky] *",
   "language": "python",
   "name": "conda-env-env_enrico-gt_rocky-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

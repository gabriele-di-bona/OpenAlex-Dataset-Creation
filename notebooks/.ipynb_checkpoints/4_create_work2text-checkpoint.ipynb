{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f499e6-b4e3-48ac-aa20-91a5f5218e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.chdir(\"../\") # resets notebook directory to repository root folder (DO ONLY ONCE!)\n",
    "import gzip\n",
    "import tqdm\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "# import pyarrow as pa\n",
    "\n",
    "def flatten(matrix):\n",
    "    return [item for row in matrix for item in row]\n",
    "\n",
    "def simpleId(text):\n",
    "    try:\n",
    "        y=text.split('/')[-1]\n",
    "    except:\n",
    "        y='NONE'\n",
    "    return y\n",
    "\n",
    "def rebuild_abstract(word_dict_full): # from the key 'abstract_inverted_index' key per paper\n",
    "    try:\n",
    "        word_dict={i:list(word_dict_full[i]) for i in word_dict_full.keys() if word_dict_full[i] is not None }\n",
    "        max_index = 0\n",
    "        for word, list_indexes in word_dict.items():\n",
    "            max_index = max([max_index]+list_indexes)\n",
    "        # create a list of elements long as the number of total words used\n",
    "        abstract = [0]*(max_index+1)\n",
    "        for i in word_dict.keys():\n",
    "            for j in word_dict[i]:\n",
    "                abstract[j] = i\n",
    "        abstract = ' '.join(abstract)\n",
    "    except:\n",
    "        abstract = 'NONE'\n",
    "    return abstract\n",
    "\n",
    "def flush_buffers(buffers, output_dir, topic_id = None):\n",
    "    # if topic_id == None, flush all, otherwise only that\n",
    "    if topic_id != None:\n",
    "        buffers_to_flush = {topic_id : buffers[topic_id]}\n",
    "    else:\n",
    "        buffers_to_flush = buffers\n",
    "    for topic_id, rows in buffers_to_flush.items():\n",
    "        # print(f\"flushing {topic_id}\",flush=True)\n",
    "        if not rows:\n",
    "            continue\n",
    "        df = pl.DataFrame(rows, orient = \"row\", schema = columns)\n",
    "\n",
    "        out_path = os.path.join(output_dir, f\"{topic_id}.csv\")\n",
    "        file_exists = os.path.isfile(out_path)\n",
    "        # # USING pandas\n",
    "        # df.write_csv(\n",
    "        #     out_path,\n",
    "        #     has_header=not file_exists,\n",
    "        #     separator=',',\n",
    "        #     append=file_exists\n",
    "        # )\n",
    "        # USING polars\n",
    "        csv_str = df.write_csv(separator=';', include_header=not file_exists)\n",
    "        with open(out_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(csv_str)\n",
    "        # flush original buffers\n",
    "        buffers[topic_id] = []\n",
    "    return buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b52ec9-2ccb-4a11-9e77-7de807615b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_subfolder = \"data/openalex-snapshot/data/works/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf4f9fb-fa7f-4414-8ef9-1f317eda9ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 424 subfolders\n"
     ]
    }
   ],
   "source": [
    "listdir=[subfolder for subfolder in sorted(os.listdir(snapshot_subfolder)) if 'updated' in subfolder]\n",
    "print(f\"Found {len(listdir)} subfolders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a04841-71ec-4f82-8513-5293f1a0b39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part_000.gz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(snapshot_subfolder+listdir[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f68ae28-b771-4a62-b7bd-b65d8c5b1a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 893 files\n"
     ]
    }
   ],
   "source": [
    "files=flatten([[snapshot_subfolder+listdir[a]+'/'+i for i in os.listdir(snapshot_subfolder+listdir[a]) if 'part' in i] for a in range(len(listdir))])\n",
    "print(f\"Found {len(files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d49b2017-b23a-4fae-aaea-fa0c684bef3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example file: data/openalex-snapshot/data/works/updated_date=2023-05-17/part_000.gz\n"
     ]
    }
   ],
   "source": [
    "print(\"Example file:\", files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ba1bb-ac6d-4be6-a1fb-9b68832e9cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 175/893 [00:31<04:07,  2.90it/s] "
     ]
    }
   ],
   "source": [
    "destination_csv_folder = \"data/works2text_by_topic_csv/\"\n",
    "os.makedirs(destination_csv_folder, exist_ok=True)\n",
    "\n",
    "# before doing any damage, check if there are files in the destination folder and stop if there are, \n",
    "# because if there are already csv, it will append and might cause many duplicates\n",
    "if os.listdir(destination_csv_folder):  # listdir returns [] if empty\n",
    "    existing = os.listdir(destination_csv_folder)\n",
    "    raise RuntimeError(\n",
    "        f\"Destination folder '{destination_csv_folder}' is not empty! \"\n",
    "        f\"Found {len(existing)} file(s), e.g., {existing[:3]}. \"\n",
    "        f\"Please clean it before running this script to avoid appending duplicates.\"\n",
    "    )\n",
    "\n",
    "columns = ['id', 'title', 'abstract']\n",
    "\n",
    "# Topic buffers: key = topic_id, value = list of rows (as dicts)\n",
    "buffers = defaultdict(list)\n",
    "BUFFER_SIZE = 1000  # flush every N rows per topic\n",
    "\n",
    "for gzfile in tqdm(files):\n",
    "    with gzip.open(gzfile, 'rt') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            if all(k in data for k in ['primary_topic', 'publication_date', 'authorships']):\n",
    "                if not (data['primary_topic'] and data['publication_date'] and data['authorships']):\n",
    "                    continue\n",
    "\n",
    "                row = {}\n",
    "                primary_topic = simpleId(data['primary_topic']['id'])\n",
    "                row['id'] = simpleId(data['id'])\n",
    "                try:\n",
    "                    row['title'] = data['title'].replace(';','.')\n",
    "                except:\n",
    "                    row['title'] = ''\n",
    "                try:\n",
    "                    row['abstract'] = rebuild_abstract(data['abstract_inverted_index']).replace(';','.')\n",
    "                except:\n",
    "                    row['abstract'] = ''\n",
    "    \n",
    "\n",
    "                buffers[primary_topic].append(row)\n",
    "\n",
    "                # Flush if buffer is too large\n",
    "                if len(buffers[primary_topic]) >= BUFFER_SIZE:\n",
    "                    buffers = flush_buffers(buffers, destination_csv_folder, primary_topic)\n",
    "\n",
    "# Final flush\n",
    "buffers = flush_buffers(buffers, destination_csv_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_enrico-gt_rocky] *",
   "language": "python",
   "name": "conda-env-env_enrico-gt_rocky-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

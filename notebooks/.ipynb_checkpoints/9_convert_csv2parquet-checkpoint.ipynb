{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ece3e34-2364-4a4c-b79a-39c4f595106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.chdir(\"../\") # resets notebook directory to repository root folder (DO ONLY ONCE!)\n",
    "import gzip\n",
    "import tqdm\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "# import pyarrow as pa\n",
    "\n",
    "def flatten(matrix):\n",
    "    return [item for row in matrix for item in row]\n",
    "\n",
    "def simpleId(text):\n",
    "    try:\n",
    "        y=text.split('/')[-1]\n",
    "    except:\n",
    "        y='NONE'\n",
    "    return y\n",
    "\n",
    "def flush_buffer(lines, output_dir):\n",
    "    df = pl.DataFrame(lines, orient = \"row\", schema = columns)\n",
    "\n",
    "    out_path = os.path.join(output_dir, f\"authors.csv\")\n",
    "    file_exists = os.path.isfile(out_path)\n",
    "    # # USING pandas\n",
    "    # df.write_csv(\n",
    "    #     out_path,\n",
    "    #     has_header=not file_exists,\n",
    "    #     separator=',',\n",
    "    #     append=file_exists\n",
    "    # )\n",
    "    # USING polars\n",
    "    csv_str = df.write_csv(separator=',', include_header=not file_exists)\n",
    "    with open(out_path, 'a', encoding='utf-8') as f:\n",
    "        f.write(csv_str)\n",
    "    lines = []\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3119ba4-9fe5-4417-a84a-93881262fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_subfolder = \"data/openalex-snapshot/data/authors/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11be39b-6a30-4091-9996-b23230f08b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 406 subfolders\n"
     ]
    }
   ],
   "source": [
    "listdir=[subfolder for subfolder in sorted(os.listdir(snapshot_subfolder)) if 'updated' in subfolder]\n",
    "print(f\"Found {len(listdir)} subfolders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bb17e5-0552-4e79-9cc8-73bae3220744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part_000.gz']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(snapshot_subfolder+listdir[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2441e2-c787-4356-bbff-f5b63127d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 530 files\n"
     ]
    }
   ],
   "source": [
    "files=flatten([[snapshot_subfolder+listdir[a]+'/'+i for i in os.listdir(snapshot_subfolder+listdir[a]) if 'part' in i] for a in range(len(listdir))])\n",
    "print(f\"Found {len(files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c3e228-d1a9-4db0-94c9-028ab96d89f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example file: data/openalex-snapshot/data/authors/updated_date=2023-06-08/part_000.gz\n"
     ]
    }
   ],
   "source": [
    "print(\"Example file:\", files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef891bd5-c9a5-4ac8-beed-b79517489ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [2:31:08<00:00, 17.11s/it]  \n"
     ]
    }
   ],
   "source": [
    "destination_csv_folder = \"data/authors/\"\n",
    "os.makedirs(destination_csv_folder, exist_ok=True)\n",
    "\n",
    "# before doing any damage, check if there are files in the destination folder and stop if there are, \n",
    "# because if there are already csv, it will append and might cause many duplicates\n",
    "if os.listdir(destination_csv_folder):  # listdir returns [] if empty\n",
    "    existing = os.listdir(destination_csv_folder)\n",
    "    raise RuntimeError(\n",
    "        f\"Destination folder '{destination_csv_folder}' is not empty! \"\n",
    "        f\"Found {len(existing)} file(s), e.g., {existing[:3]}. \"\n",
    "        f\"Please clean it before running this script to avoid appending duplicates.\"\n",
    "    )\n",
    "\n",
    "columns=['id', 'display_name','orcid','works_count','h_index','cited_by_count','topics','affiliations','works_count_by_year','cited_by_count_by_year']\n",
    "for gzfile in tqdm(files):\n",
    "    lines=[]\n",
    "    with gzip.open(gzfile, 'rt') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if data['orcid'] is None or len(data['orcid']) == 0:\n",
    "                    orcid = ''\n",
    "                else:\n",
    "                    orcid = data['orcid'].split('https://orcid.org/')[-1]\n",
    "                ll=[simpleId(data['id']),data['display_name'],orcid,int(data['works_count']),\n",
    "                    int(data['summary_stats']['h_index']),int(data['summary_stats']['cited_by_count'])\n",
    "                    ]\n",
    "                # topics\n",
    "                try:\n",
    "                     ll.append(';'.join([simpleId(i['id'])+'_'+str(i['count']) for i in data['topics']]))\n",
    "                except:\n",
    "                     ll.append('')\n",
    "                # institutions\n",
    "                try:\n",
    "                    ll.append(';'.join([simpleId(i['institution']['id'])+'_'+simpleId(i['institution']['country_code'])+'_'+'_'.join([str(j) for j in i['years']]) for i in data['affiliations']]))\n",
    "                except:\n",
    "                    # if not available, check if last institution is present (does not have years)\n",
    "                    try:\n",
    "                        ll.append(';'.join([simpleId(i['id'])+'_'+i['country_code'] for i in data['last_known_institutions']]))\n",
    "                    except:\n",
    "                        try:\n",
    "                            ll.append(simpleId(data['last_known_institution']['id'])+'_'+data['last_known_institution']['country_code'])\n",
    "                        except:\n",
    "                            ll.append('')\n",
    "                # works_count by year\n",
    "                ll.append(';'.join([str(i['works_count'])+'_'+str(i['year']) for i in data['counts_by_year'] if i['works_count'] > 0]))\n",
    "                # cited_by_count by year\n",
    "                ll.append(';'.join([str(i['cited_by_count'])+'_'+str(i['year']) for i in data['counts_by_year'] if i['cited_by_count'] > 0]))\n",
    "                \n",
    "                lines.append(ll)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Errore nel parsing della riga: {e}\")\n",
    "            if len(lines) > 1000:\n",
    "                lines = flush_buffer(lines, destination_csv_folder)\n",
    "lines = flush_buffer(lines, destination_csv_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_enrico-gt_rocky] *",
   "language": "python",
   "name": "conda-env-env_enrico-gt_rocky-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

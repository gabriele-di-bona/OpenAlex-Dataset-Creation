{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd57538-3fd9-4b01-8558-00e2b49ec0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.chdir(\"../\") # resets notebook directory to repository root folder (DO ONLY ONCE!)\n",
    "import gzip\n",
    "import tqdm\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "# import pyarrow as pa\n",
    "\n",
    "def flatten(matrix):\n",
    "    return [item for row in matrix for item in row]\n",
    "\n",
    "def simpleId(text):\n",
    "    try:\n",
    "        y=text.split('/')[-1]\n",
    "    except:\n",
    "        y='NONE'\n",
    "    return y\n",
    "\n",
    "def flush_buffers(buffers, output_dir, topic_id = None):\n",
    "    # if topic_id == None, flush all, otherwise only that\n",
    "    if topic_id != None:\n",
    "        buffers_to_flush = {topic_id : buffers[topic_id]}\n",
    "    else:\n",
    "        buffers_to_flush = buffers\n",
    "    for topic_id, rows in buffers_to_flush.items():\n",
    "        # print(f\"flushing {topic_id}\",flush=True)\n",
    "        if not rows:\n",
    "            continue\n",
    "        df = pl.DataFrame(rows, orient = \"row\", schema = columns)\n",
    "\n",
    "        out_path = os.path.join(output_dir, f\"{topic_id}.csv\")\n",
    "        file_exists = os.path.isfile(out_path)\n",
    "        # # USING pandas\n",
    "        # df.write_csv(\n",
    "        #     out_path,\n",
    "        #     has_header=not file_exists,\n",
    "        #     separator=',',\n",
    "        #     append=file_exists\n",
    "        # )\n",
    "        # USING polars\n",
    "        csv_str = df.write_csv(separator=',', include_header=not file_exists)\n",
    "        with open(out_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(csv_str)\n",
    "        # flush original buffers\n",
    "        buffers[topic_id] = []\n",
    "    return buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3119ba4-9fe5-4417-a84a-93881262fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_subfolder = \"data/openalex-snapshot/data/works/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11be39b-6a30-4091-9996-b23230f08b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 424 subfolders\n"
     ]
    }
   ],
   "source": [
    "listdir=[subfolder for subfolder in sorted(os.listdir(snapshot_subfolder)) if 'updated' in subfolder]\n",
    "print(f\"Found {len(listdir)} subfolders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bb17e5-0552-4e79-9cc8-73bae3220744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part_000.gz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(snapshot_subfolder+listdir[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2441e2-c787-4356-bbff-f5b63127d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 893 files\n"
     ]
    }
   ],
   "source": [
    "files=flatten([[snapshot_subfolder+listdir[a]+'/'+i for i in os.listdir(snapshot_subfolder+listdir[a]) if 'part' in i] for a in range(len(listdir))])\n",
    "print(f\"Found {len(files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c3e228-d1a9-4db0-94c9-028ab96d89f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example file: data/openalex-snapshot/data/works/updated_date=2023-05-17/part_000.gz\n"
     ]
    }
   ],
   "source": [
    "print(\"Example file:\", files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477d5046-a8e4-46fe-a214-f396d5013fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 182/893 [02:06<08:16,  1.43it/s] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[38;5;66;03m# Flush if buffer is too large\u001b[39;00m\n\u001b[1;32m    116\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffers[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_topic\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m BUFFER_SIZE:\n\u001b[0;32m--> 117\u001b[0m                     buffers \u001b[38;5;241m=\u001b[39m \u001b[43mflush_buffers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination_csv_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprimary_topic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Final flush\u001b[39;00m\n\u001b[1;32m    120\u001b[0m buffers \u001b[38;5;241m=\u001b[39m flush_buffers(buffers, destination_csv_folder)\n",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m, in \u001b[0;36mflush_buffers\u001b[0;34m(buffers, output_dir, topic_id)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rows:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m df \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mlines\u001b[49m, orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m, schema \u001b[38;5;241m=\u001b[39m columns)\n\u001b[1;32m     33\u001b[0m out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m file_exists \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(out_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lines' is not defined"
     ]
    }
   ],
   "source": [
    "destination_csv_folder = \"data/works_by_topic_csv/\"\n",
    "os.makedirs(destination_csv_folder, exist_ok=True)\n",
    "\n",
    "# before doing any damage, check if there are files in the destination folder and stop if there are, \n",
    "# because if there are already csv, it will append and might cause many duplicates\n",
    "if os.listdir(destination_csv_folder):  # listdir returns [] if empty\n",
    "    existing = os.listdir(destination_csv_folder)\n",
    "    raise RuntimeError(\n",
    "        f\"Destination folder '{destination_csv_folder}' is not empty! \"\n",
    "        f\"Found {len(existing)} file(s), e.g., {existing[:3]}. \"\n",
    "        f\"Please clean it before running this script to avoid appending duplicates.\"\n",
    "    )\n",
    "\n",
    "columns = ['id', 'date', 'type', 'language', 'journal', 'doi', 'authors', 'topics', 'references', \n",
    "           'sdg', 'keywords', 'grants', 'primary_topic']\n",
    "\n",
    "# Topic buffers: key = topic_id, value = list of rows (as dicts)\n",
    "buffers = defaultdict(list)\n",
    "BUFFER_SIZE = 1000  # flush every N rows per topic\n",
    "\n",
    "for gzfile in tqdm(files):\n",
    "    with gzip.open(gzfile, 'rt') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            if all(k in data for k in ['primary_topic', 'publication_date', 'authorships']):\n",
    "                if not (data['primary_topic'] and data['publication_date'] and data['authorships']):\n",
    "                    continue\n",
    "\n",
    "                row = {}\n",
    "                row['primary_topic'] = simpleId(data['primary_topic']['id'])\n",
    "                row['id'] = simpleId(data['id'])\n",
    "                row['date'] = data['publication_date']\n",
    "                row['type'] = data.get('type', '')\n",
    "\n",
    "                try:\n",
    "                    row['journal'] = simpleId(data['primary_location']['source']['id'])\n",
    "                except:\n",
    "                    row['journal'] = ''\n",
    "\n",
    "                # Authors: authorID_institutionID1|institutionID2_corresponding;...\n",
    "                try:\n",
    "                    author_blocks = []\n",
    "                    for a in data['authorships']:\n",
    "                        author_id = simpleId(a['author']['id'])\n",
    "                \n",
    "                        # Get all institution IDs\n",
    "                        if 'institutions' in a and a['institutions']:\n",
    "                            inst_ids = [simpleId(inst['id']) for inst in a['institutions'] if 'id' in inst]\n",
    "                            inst_str = '|'.join(inst_ids)\n",
    "                        else:\n",
    "                            inst_str = ''\n",
    "                \n",
    "                        is_corr = str(a.get('is_corresponding', False))[0]  # T/F\n",
    "                \n",
    "                        author_blocks.append(f\"{author_id}_{inst_str}_{is_corr}\")\n",
    "                \n",
    "                    row['authors'] = ';'.join(author_blocks)\n",
    "                except:\n",
    "                    row['authors'] = ''\n",
    "\n",
    "\n",
    "                # Topics\n",
    "                try:\n",
    "                    row['topics'] = ';'.join([f\"{simpleId(t['id'])}_{t['score']}\" for t in data['topics']])\n",
    "                except:\n",
    "                    row['topics'] = row['primary_topic']\n",
    "\n",
    "                # References\n",
    "                try:\n",
    "                    row['references'] = ';'.join([simpleId(r) for r in data['referenced_works']])\n",
    "                except:\n",
    "                    row['references'] = ''\n",
    "\n",
    "                # Sustainable Development Goals\n",
    "                try:\n",
    "                    sdg_list = data.get('sustainable_development_goals', [])\n",
    "                    row['sdg'] = ';'.join([\n",
    "                        f\"{simpleId(sdg['id'])}_{sdg['score']}\" \n",
    "                        for sdg in sdg_list if 'id' in sdg and 'score' in sdg\n",
    "                    ])\n",
    "                except:\n",
    "                    row['sdg'] = ''\n",
    "                \n",
    "                # Keywords\n",
    "                try:\n",
    "                    kw = data.get('keywords', [])\n",
    "                    row['keywords'] = ';'.join([\n",
    "                        f\"{kwd['display_name'].replace('_',' ').replace(';',' ')}_{kwd['score']}\"\n",
    "                        for kwd in kw if 'display_name' in kwd and 'score' in kwd\n",
    "                    ])\n",
    "                except:\n",
    "                    row['keywords'] = ''\n",
    "                \n",
    "                # Grants\n",
    "                try:\n",
    "                    grants = data.get('grants', [])\n",
    "                    row['grants'] = ';'.join([\n",
    "                        f\"{grant['award_id']}_{simpleId(grant['funder'])}\"\n",
    "                        for grant in grants if 'award_id' in grant and 'funder' in grant\n",
    "                    ])\n",
    "                except:\n",
    "                    row['grants'] = ''\n",
    "\n",
    "                # DOI (extract only the identifier part, not the full https://doi.org/...)\n",
    "                try:\n",
    "                    row['doi'] = data['doi'].replace('https://doi.org/', '')\n",
    "                except:\n",
    "                    row['doi'] = ''\n",
    "                \n",
    "                # Language (ISO code, e.g., 'en', 'fr')\n",
    "                row['language'] = data.get('language', '')\n",
    "\n",
    "                buffers[row['primary_topic']].append(row)\n",
    "\n",
    "                # Flush if buffer is too large\n",
    "                if len(buffers[row['primary_topic']]) >= BUFFER_SIZE:\n",
    "                    buffers = flush_buffers(buffers, destination_csv_folder, row['primary_topic'])\n",
    "\n",
    "# Final flush\n",
    "buffers = flush_buffers(buffers, destination_csv_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_enrico-gt_rocky] *",
   "language": "python",
   "name": "conda-env-env_enrico-gt_rocky-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

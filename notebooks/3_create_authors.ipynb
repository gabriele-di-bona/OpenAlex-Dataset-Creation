{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ece3e34-2364-4a4c-b79a-39c4f595106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.chdir(\"../\") # resets notebook directory to repository root folder (DO ONLY ONCE!)\n",
    "import gzip\n",
    "import tqdm\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "# import pyarrow as pa\n",
    "\n",
    "def flatten(matrix):\n",
    "    return [item for row in matrix for item in row]\n",
    "\n",
    "def simpleId(text):\n",
    "    try:\n",
    "        y=text.split('/')[-1]\n",
    "    except:\n",
    "        y='NONE'\n",
    "    return y\n",
    "\n",
    "def flush_buffer(lines, output_dir):\n",
    "    df = pl.DataFrame(lines, orient = \"row\", schema = columns)\n",
    "\n",
    "    out_path = os.path.join(output_dir, f\"authors.csv\")\n",
    "    file_exists = os.path.isfile(out_path)\n",
    "    # # USING pandas\n",
    "    # df.write_csv(\n",
    "    #     out_path,\n",
    "    #     has_header=not file_exists,\n",
    "    #     separator=',',\n",
    "    #     append=file_exists\n",
    "    # )\n",
    "    # USING polars\n",
    "    csv_str = df.write_csv(separator=',', include_header=not file_exists)\n",
    "    with open(out_path, 'a', encoding='utf-8') as f:\n",
    "        f.write(csv_str)\n",
    "    lines = []\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3119ba4-9fe5-4417-a84a-93881262fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_subfolder = \"data/openalex-snapshot/data/authors/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11be39b-6a30-4091-9996-b23230f08b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 subfolders\n"
     ]
    }
   ],
   "source": [
    "listdir=[subfolder for subfolder in sorted(os.listdir(snapshot_subfolder)) if 'updated' in subfolder]\n",
    "print(f\"Found {len(listdir)} subfolders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bb17e5-0552-4e79-9cc8-73bae3220744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part_0219.gz', 'part_0116.gz', 'part_0302.gz', 'part_0279.gz', 'part_0201.gz', 'part_0211.gz', 'part_0235.gz', 'part_0215.gz', 'part_0191.gz', 'part_0065.gz', 'part_0172.gz', 'part_0103.gz', 'part_0009.gz', 'part_0052.gz', 'part_0255.gz', 'part_0026.gz', 'part_0227.gz', 'part_0231.gz', 'part_0193.gz', 'part_0264.gz', 'part_0159.gz', 'part_0155.gz', 'part_0095.gz', 'part_0045.gz', 'part_0107.gz', 'part_0237.gz', 'part_0126.gz', 'part_0106.gz', 'part_0229.gz', 'part_0307.gz', 'part_0256.gz', 'part_0016.gz', 'part_0138.gz', 'part_0147.gz', 'part_0290.gz', 'part_0173.gz', 'part_0079.gz', 'part_0068.gz', 'part_0165.gz', 'part_0060.gz', 'part_0177.gz', 'part_0120.gz', 'part_0309.gz', 'part_0294.gz', 'part_0198.gz', 'part_0028.gz', 'part_0241.gz', 'part_0259.gz', 'part_0129.gz', 'part_0166.gz', 'part_0266.gz', 'part_0272.gz', 'part_0301.gz', 'part_0300.gz', 'part_0073.gz', 'part_0282.gz', 'part_0153.gz', 'part_0158.gz', 'part_0130.gz', 'part_0144.gz', 'part_0233.gz', 'part_0143.gz', 'part_0039.gz', 'part_0303.gz', 'part_0084.gz', 'part_0128.gz', 'part_0218.gz', 'part_0305.gz', 'part_0223.gz', 'part_0226.gz', 'part_0225.gz', 'part_0249.gz', 'part_0196.gz', 'part_0002.gz', 'part_0019.gz', 'part_0044.gz', 'part_0251.gz', 'part_0134.gz', 'part_0262.gz', 'part_0273.gz', 'part_0160.gz', 'part_0190.gz', 'part_0180.gz', 'part_0011.gz', 'part_0101.gz', 'part_0127.gz', 'part_0056.gz', 'part_0156.gz', 'part_0178.gz', 'part_0007.gz', 'part_0108.gz', 'part_0008.gz', 'part_0012.gz', 'part_0023.gz', 'part_0312.gz', 'part_0283.gz', 'part_0119.gz', 'part_0140.gz', 'part_0299.gz', 'part_0252.gz', 'part_0280.gz', 'part_0086.gz', 'part_0174.gz', 'part_0030.gz', 'part_0135.gz', 'part_0183.gz', 'part_0306.gz', 'part_0069.gz', 'part_0274.gz', 'part_0092.gz', 'part_0293.gz', 'part_0188.gz', 'part_0051.gz', 'part_0014.gz', 'part_0121.gz', 'part_0284.gz', 'part_0146.gz', 'part_0167.gz', 'part_0055.gz', 'part_0311.gz', 'part_0192.gz', 'part_0181.gz', 'part_0292.gz', 'part_0214.gz', 'part_0057.gz', 'part_0261.gz', 'part_0041.gz', 'part_0040.gz', 'part_0289.gz', 'part_0152.gz', 'part_0124.gz', 'part_0275.gz', 'part_0104.gz', 'part_0027.gz', 'part_0221.gz', 'part_0132.gz', 'part_0074.gz', 'part_0314.gz', 'part_0022.gz', 'part_0149.gz', 'part_0015.gz', 'part_0228.gz', 'part_0169.gz', 'part_0310.gz', 'part_0035.gz', 'part_0077.gz', 'part_0111.gz', 'part_0031.gz', 'part_0020.gz', 'part_0090.gz', 'part_0001.gz', 'part_0105.gz', 'part_0288.gz', 'part_0194.gz', 'part_0046.gz', 'part_0115.gz', 'part_0024.gz', 'part_0110.gz', 'part_0102.gz', 'part_0254.gz', 'part_0093.gz', 'part_0265.gz', 'part_0151.gz', 'part_0085.gz', 'part_0296.gz', 'part_0206.gz', 'part_0081.gz', 'part_0268.gz', 'part_0238.gz', 'part_0213.gz', 'part_0287.gz', 'part_0217.gz', 'part_0042.gz', 'part_0047.gz', 'part_0209.gz', 'part_0176.gz', 'part_0187.gz', 'part_0295.gz', 'part_0032.gz', 'part_0207.gz', 'part_0000.gz', 'part_0304.gz', 'part_0100.gz', 'part_0257.gz', 'part_0080.gz', 'part_0222.gz', 'part_0197.gz', 'part_0109.gz', 'part_0260.gz', 'part_0082.gz', 'part_0053.gz', 'part_0131.gz', 'part_0072.gz', 'part_0246.gz', 'part_0234.gz', 'part_0195.gz', 'part_0186.gz', 'part_0089.gz', 'part_0098.gz', 'part_0224.gz', 'part_0278.gz', 'part_0062.gz', 'part_0239.gz', 'part_0161.gz', 'part_0199.gz', 'part_0313.gz', 'part_0037.gz', 'part_0202.gz', 'part_0164.gz', 'part_0247.gz', 'part_0242.gz', 'part_0203.gz', 'part_0240.gz', 'part_0048.gz', 'part_0034.gz', 'part_0071.gz', 'part_0136.gz', 'part_0148.gz', 'part_0220.gz', 'part_0063.gz', 'part_0059.gz', 'part_0139.gz', 'part_0029.gz', 'part_0021.gz', 'part_0184.gz', 'part_0170.gz', 'part_0185.gz', 'part_0263.gz', 'part_0050.gz', 'part_0269.gz', 'part_0097.gz', 'part_0308.gz', 'part_0091.gz', 'part_0281.gz', 'part_0125.gz', 'part_0112.gz', 'part_0168.gz', 'part_0232.gz', 'part_0018.gz', 'part_0076.gz', 'part_0210.gz', 'part_0049.gz', 'part_0003.gz', 'part_0010.gz', 'part_0250.gz', 'part_0033.gz', 'part_0088.gz', 'part_0099.gz', 'part_0154.gz', 'part_0118.gz', 'part_0157.gz', 'part_0205.gz', 'part_0123.gz', 'part_0096.gz', 'part_0067.gz', 'part_0277.gz', 'part_0061.gz', 'part_0025.gz', 'part_0189.gz', 'part_0137.gz', 'part_0204.gz', 'part_0114.gz', 'part_0117.gz', 'part_0243.gz', 'part_0113.gz', 'part_0066.gz', 'part_0141.gz', 'part_0083.gz', 'part_0248.gz', 'part_0043.gz', 'part_0271.gz', 'part_0145.gz', 'part_0258.gz', 'part_0291.gz', 'part_0182.gz', 'part_0216.gz', 'part_0179.gz', 'part_0150.gz', 'part_0175.gz', 'part_0285.gz', 'part_0013.gz', 'part_0244.gz', 'part_0054.gz', 'part_0078.gz', 'part_0200.gz', 'part_0212.gz', 'part_0286.gz', 'part_0253.gz', 'part_0058.gz', 'part_0270.gz', 'part_0133.gz', 'part_0163.gz', 'part_0036.gz', 'part_0017.gz', 'part_0297.gz', 'part_0298.gz', 'part_0276.gz', 'part_0142.gz', 'part_0005.gz', 'part_0171.gz', 'part_0006.gz', 'part_0208.gz', 'part_0004.gz', 'part_0094.gz', 'part_0162.gz', 'part_0087.gz', 'part_0070.gz', 'part_0267.gz', 'part_0075.gz', 'part_0236.gz', 'part_0064.gz', 'part_0245.gz', 'part_0122.gz', 'part_0230.gz', 'part_0038.gz']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(snapshot_subfolder+listdir[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2441e2-c787-4356-bbff-f5b63127d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 413 files\n"
     ]
    }
   ],
   "source": [
    "files=flatten([[snapshot_subfolder+listdir[a]+'/'+i for i in os.listdir(snapshot_subfolder+listdir[a]) if 'part' in i] for a in range(len(listdir))])\n",
    "print(f\"Found {len(files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c3e228-d1a9-4db0-94c9-028ab96d89f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example file: data/openalex-snapshot/data/authors/updated_date=2026-02-01/part_0219.gz\n"
     ]
    }
   ],
   "source": [
    "print(\"Example file:\", files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef891bd5-c9a5-4ac8-beed-b79517489ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 413/413 [4:53:21<00:00, 42.62s/it]   \n"
     ]
    }
   ],
   "source": [
    "destination_csv_folder = \"data/authors/\"\n",
    "os.makedirs(destination_csv_folder, exist_ok=True)\n",
    "\n",
    "# before doing any damage, check if there are files in the destination folder and stop if there are, \n",
    "# because if there are already csv, it will append and might cause many duplicates\n",
    "if os.listdir(destination_csv_folder):  # listdir returns [] if empty\n",
    "    existing = os.listdir(destination_csv_folder)\n",
    "    raise RuntimeError(\n",
    "        f\"Destination folder '{destination_csv_folder}' is not empty! \"\n",
    "        f\"Found {len(existing)} file(s), e.g., {existing[:3]}. \"\n",
    "        f\"Please clean it before running this script to avoid appending duplicates.\"\n",
    "    )\n",
    "\n",
    "columns=['id', 'display_name','orcid','works_count','h_index','cited_by_count','topics','affiliations','works_count_by_year','cited_by_count_by_year']\n",
    "lines=[]\n",
    "for gzfile in tqdm(files):\n",
    "    with gzip.open(gzfile, 'rt') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if \"orcid\" not in data or data['orcid'] is None or len(data['orcid']) == 0:\n",
    "                    orcid = ''\n",
    "                else:\n",
    "                    orcid = data['orcid'].split('https://orcid.org/')[-1]\n",
    "                ll=[simpleId(data['id']),data['display_name'],orcid,int(data['works_count']),\n",
    "                    int(data['summary_stats']['h_index']),int(data['cited_by_count'])\n",
    "                    ]\n",
    "                # topics\n",
    "                try:\n",
    "                     ll.append(';'.join([simpleId(i['id'])+'_'+str(i['count']) for i in data['topics']]))\n",
    "                except:\n",
    "                     ll.append('')\n",
    "                # institutions\n",
    "                try:\n",
    "                    ll.append(';'.join([simpleId(i['institution']['id'])+'_'+simpleId(i['institution']['country_code'])+'_'+'_'.join([str(j) for j in i['years']]) for i in data['affiliations']]))\n",
    "                except:\n",
    "                    # if not available, check if last institution is present (does not have years)\n",
    "                    try:\n",
    "                        ll.append(';'.join([simpleId(i['id'])+'_'+i['country_code'] for i in data['last_known_institutions']]))\n",
    "                    except:\n",
    "                        try:\n",
    "                            ll.append(simpleId(data['last_known_institution']['id'])+'_'+data['last_known_institution']['country_code'])\n",
    "                        except:\n",
    "                            ll.append('')\n",
    "                # works_count by year\n",
    "                ll.append(';'.join([str(i['works_count'])+'_'+str(i['year']) for i in data['counts_by_year'] if i['works_count'] > 0]))\n",
    "                # cited_by_count by year\n",
    "                ll.append(';'.join([str(i['cited_by_count'])+'_'+str(i['year']) for i in data['counts_by_year'] if i['cited_by_count'] > 0]))\n",
    "                \n",
    "                lines.append(ll)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Errore nel parsing della riga: {e}\")\n",
    "            if len(lines) > 1000:\n",
    "                lines = flush_buffer(lines, destination_csv_folder)\n",
    "lines = flush_buffer(lines, destination_csv_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_enrico-gt_rocky] *",
   "language": "python",
   "name": "conda-env-env_enrico-gt_rocky-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

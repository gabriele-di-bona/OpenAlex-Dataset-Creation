{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d963d08-05d7-4819-b935-54e8966e872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\") # resets notebook directory to repository root folder (DO ONLY ONCE!)\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pypq\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9dbc41b-c30e-47e0-8e8e-505288b47995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyArrow strings!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Add utils directory in the list of directories to look for packages to import\n",
    "sys.path.insert(0, os.path.join(os.getcwd(),'utils'))\n",
    "from read_parquet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78dc3fa3-caf5-4a79-a060-cb0c3d1e1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "works_by_topic_parquet_folder = \"data/works_by_topic_parquet/\"\n",
    "works2references_by_topic_parquet_folder = \"data/works2references_by_topic_parquet/\"\n",
    "works2citations_by_topic_csv_folder = \"data/works2citations_by_topic_csv/\"\n",
    "os.makedirs(works2citations_by_topic_csv_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d2d7ac-473b-4122-9157-fa0fd95cc99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush_buffers(buffers, output_dir, topic_id=None):\n",
    "    if topic_id is not None:\n",
    "        buffers_to_flush = {topic_id: buffers[topic_id]}\n",
    "    else:\n",
    "        buffers_to_flush = buffers\n",
    "\n",
    "    for topic_id, rows in buffers_to_flush.items():\n",
    "        if not rows:\n",
    "            continue\n",
    "        df = pl.DataFrame(rows, orient=\"row\", schema=columns)\n",
    "        out_path = os.path.join(output_dir, f\"{topic_id}.csv\")\n",
    "        file_exists = os.path.isfile(out_path)\n",
    "        csv_str = df.write_csv(separator=DELIMITER, include_header=not file_exists)\n",
    "        with open(out_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(csv_str)\n",
    "        buffers[topic_id] = []\n",
    "\n",
    "    return buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7537ce20-acd7-41ad-a7ec-eed2e5183ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4516"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = len(os.listdir(works_by_topic_parquet_folder))\n",
    "num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "827e7f7f-bf8c-4955-9211-b262fe351f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently we have 109 topics processed for references\n"
     ]
    }
   ],
   "source": [
    "topics = [topic[:-8] for topic in os.listdir(works2references_by_topic_parquet_folder)]\n",
    "\n",
    "print(\"currently we have\", len(topics), \"topics processed for references\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e86c803-cec1-44e1-868b-0c8e7368fb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104d0cce03aa40129c6316bdf83ad567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently we have 357 topics processed for references\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a484e9ba25b48789edc3e8c5c0de7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently we have 1004 topics processed for references\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6380ef2bbf4bc689f837a09603e2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1004 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently we have 2535 topics processed for references\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79dc3abf0ff24685a4377fe146eb6db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently we have 4516 topics processed for references\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 5000\n",
    "DELIMITER = ','\n",
    "\n",
    "# OUTPUT SCHEMA\n",
    "columns = [\n",
    "    \"work_id\", \"primary_topic\", \"publication_date\",\n",
    "    \"referenced_work_id\", \"referenced_primary_topic\", \"referenced_publication_date\"\n",
    "]\n",
    "\n",
    "# before doing any damage, check if there are files in the destination folder and stop if there are, \n",
    "# because if there are already csv, it will append and might cause many duplicates\n",
    "if os.listdir(works2citations_by_topic_csv_folder):  # listdir returns [] if empty\n",
    "    existing = os.listdir(works2citations_by_topic_csv_folder)\n",
    "    raise RuntimeError(\n",
    "        f\"Destination folder '{works2citations_by_topic_csv_folder}' is not empty! \"\n",
    "        f\"Found {len(existing)} file(s), e.g., {existing[:3]}. \"\n",
    "        f\"Please clean it before running this script to avoid appending duplicates.\"\n",
    "    )\n",
    "\n",
    "# Buffers: cited_topic â†’ list of dicts\n",
    "buffers = {}\n",
    "\n",
    "processed_topics = []\n",
    "# Process each topic-level reference file\n",
    "while len(processed_topics) < num_topics: # this while is useful if references are being processed at the same time as citations\n",
    "    for topic in tqdm(topics):\n",
    "        if topic not in processed_topics:\n",
    "            file = works2references_by_topic_parquet_folder+topic+\".parquet\"\n",
    "            df = pl.read_parquet(file)\n",
    "        \n",
    "            for row in df.iter_rows(named=True):\n",
    "                work_id = row[\"work_id\"]\n",
    "                primary_topic = row[\"primary_topic\"]\n",
    "                publication_date = row[\"publication_date\"]\n",
    "                referenced_work_id = row[\"referenced_work_id\"]\n",
    "                referenced_primary_topic = row[\"referenced_primary_topic\"]\n",
    "                referenced_publication_date = row[\"referenced_publication_date\"]\n",
    "        \n",
    "                # Skip if reference target is unknown\n",
    "                if not referenced_primary_topic or not primary_topic:\n",
    "                    continue\n",
    "        \n",
    "                rec = [work_id,primary_topic,publication_date,referenced_work_id,referenced_primary_topic,referenced_publication_date]\n",
    "        \n",
    "                buffers.setdefault(referenced_primary_topic, []).append(rec)\n",
    "        \n",
    "                if len(buffers[referenced_primary_topic]) >= BUFFER_SIZE:\n",
    "                    flush_buffers(buffers, output_dir = works2citations_by_topic_csv_folder, topic_id=referenced_primary_topic)\n",
    "            processed_topics.append(topic)\n",
    "    topics = [topic[:-8] for topic in os.listdir(works2references_by_topic_parquet_folder)]\n",
    "    print(\"currently we have\", len(topics), \"topics processed for references\")\n",
    "# Final flush\n",
    "buffers = flush_buffers(buffers, output_dir = works2citations_by_topic_csv_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_enrico-gt_rocky] *",
   "language": "python",
   "name": "conda-env-env_enrico-gt_rocky-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

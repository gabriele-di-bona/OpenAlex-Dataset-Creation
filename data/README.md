# `~/data/` — OpenAlex Processed Data Folder

This folder stores all intermediate and final data tables created during the processing of the Release 2026-01-15 OpenAlex snapshot.

**Schema note:** recent OpenAlex snapshots include both the new schema and a **legacy** schema. The download notebook now **skips legacy-data** and fetches only the new-schema `data/` tree. The notebooks in this repository use only the new schema going forward.

> ⚠️ **Note:** All contents in this folder are ignored by Git (via `.gitignore`) to avoid uploading large datasets.  
> This `README.md` is tracked to ensure the folder is preserved in version control.

---

## Folder Purpose

The `data/` folder contains:

- The downloaded OpenAlex snapshot (`~/data/openalex-snapshot/`)
- All intermediate `.csv` tables
- The final `.parquet` versions of each processed table
- Topic-split outputs (e.g., works, text, authors, citations)
- Flat tables (e.g., funders, awards, institutions, authors)

---

## Subfolders (after processing)

| Path                                   | Description                                                  |
|----------------------------------------|--------------------------------------------------------------|
| `openalex-snapshot/`                   | Original JSON files from OpenAlex (~703G, new-schema `data/`) |
| `works_by_topic_csv/`                  | Raw CSV files, grouped by primary topic                      |
| `works_by_topic_parquet/`              | Optimized `.parquet` version of the same                     |
| `authors/`                             | Author tables (CSV + `.parquet`)                             |
| `institutions/`                        | Institution tables (CSV + `.parquet`)                        |
| `funders/`                             | Funder tables (CSV + `.parquet`)                             |
| `awards/`                              | Awards tables (CSV + `.parquet`)                             |
| `topics/`                              | Topics metadata table (CSV + `.parquet`)                     |
| `works2text_by_topic_csv/`             | Raw CSVs with work titles and abstracts                      |
| `works2text_by_topic_parquet/`         | Compressed `.parquet` equivalents                            |
| `works2references_by_topic_parquet/`   | Work-to-reference edges (parquet, exploded)                  |
| `works2citations_by_topic_csv/`        | Work-to-citation tables by **cited** topic (CSV format)      |
| `works2citations_by_topic_parquet/`    | Work-to-citation tables by **cited** topic (parquet format)  |
| `works2related_by_topic_parquet/`      | Work-to-related edges (parquet, exploded)                    |
| `works2year_by_topic_parquet/`         | Per-topic work counts per year                               |
| `works2topic_by_topic_parquet/`        | Top-3 topic assignment per work                              |
| `author2work_by_topic_parquet/`        | Author-to-work link table (by topic)                         |
| `all_works2primary_topic_parquet/`     | Flat table mapping every work to its primary topic           |
| `ai_database/`                         | AI subfield outputs (topics, keywords, filtered works)       |

---

## Git Ignore Setup

To avoid uploading large files to GitHub:

- The `.gitignore` in the repository root includes:
```
data/*
!data/README.md
```

This ensures:
- All dataset files are ignored
- The folder itself is retained in Git
- This `README.md` explains what goes here

---

## Rebuilding the Data

If needed, the entire folder can be regenerated by following the instructions in the main `README.md`, starting from:

```
~/notebooks/1_download_snapshot.ipynb
```

and continuing with all subsequent numbered notebooks and conversion steps.
